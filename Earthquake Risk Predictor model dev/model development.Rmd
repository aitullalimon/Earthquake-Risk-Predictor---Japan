---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
"Model Development in RStudio"
#Model-1
library(tidyverse)
library(lubridate)

data <- read_csv("/Users/aitullalabiblimon/Documents/MP-4/Japan earthquakes 2001 - 2018.csv")
```
```{r}
data_prepped <- data %>%
  mutate(
    risk_level = case_when(
      mag >= 6 ~ "High",
      mag >= 4.5 ~ "Medium",
      TRUE ~ "Low"
    ),
    risk_level = factor(risk_level, levels = c("Low", "Medium", "High")),
    year = year(time)
  ) %>%
  select(time, latitude, longitude, depth, mag, place, year, risk_level)
```
```{r}
library(caret)
set.seed(123)

splitIndex <- createDataPartition(data_prepped$risk_level, p = 0.8, list = FALSE)
trainData <- data_prepped[splitIndex, ]
testData  <- data_prepped[-splitIndex, ]
```
```{r}
table(data_prepped$risk_level)
```
```{r}
data_prepped <- data %>%
  mutate(
    risk_level = case_when(
      mag >= 6.0 ~ "High",
      mag >= 5.0 ~ "Medium",
      TRUE ~ "Low"
    ),
    risk_level = factor(risk_level, levels = c("Low", "Medium", "High")),
    year = year(time)
  )
```
```{r}
install.packages("smotefamily")
library(smotefamily)
library(tidyverse)
library(caret)

# Convert to numeric target (as required by smotefamily)
trainData$risk_level_num <- as.numeric(trainData$risk_level)

# Extract features and labels
features <- trainData %>% select(latitude, longitude, depth, mag, year)
labels <- trainData$risk_level_num
```
```{r}
smote_out <- SMOTE(X = features, target = labels, K = 5)

# Combine the balanced data
balanced_train <- smote_out$data
balanced_train$risk_level <- factor(balanced_train$class, 
                                    levels = 1:3, 
                                    labels = c("Low", "Medium", "High"))
```

```{r}
table(balanced_train$risk_level)
```
```{r}
#install.packages("tidyverse")
#install.packages("randomForest")
#install.packages("smotefamily")
#install.packages("caret")

library(tidyverse)
library(randomForest)
library(smotefamily)
library(caret)

```
```{r}
data <- read_csv("/Users/aitullalabiblimon/Documents/MP-4/Japan earthquakes 2001 - 2018.csv")

# Select key features
data_prepped <- data %>%
  mutate(
    year = lubridate::year(time),
    risk_level = ifelse(mag >= 5, "High", "Medium")
  ) %>%
  select(latitude, longitude, depth, mag, year, risk_level) %>%
  drop_na()

# Convert to factor
data_prepped$risk_level <- as.factor(data_prepped$risk_level)

```
```{r}
set.seed(123)
splitIndex <- createDataPartition(data_prepped$risk_level, p = 0.8, list = FALSE)
trainData <- data_prepped[splitIndex, ]
testData  <- data_prepped[-splitIndex, ]

```
```{r}
train_features <- trainData %>% select(latitude, longitude, depth, mag, year)
train_labels <- trainData$risk_level

# Encode target to numeric for SMOTE
train_labels_num <- as.numeric(train_labels)

# Apply SMOTE
smote_result <- SMOTE(X = train_features, target = train_labels_num, K = 5)
balanced_train <- smote_result$data
colnames(balanced_train)[ncol(balanced_train)] <- "risk_level"

# Convert to factor
balanced_train$risk_level <- factor(balanced_train$risk_level, levels = c(1, 2, 3), labels = c("Low", "Medium", "High"))
```

```{r}
table(balanced_train$risk_level)
str(balanced_train)
```
```{r}
# Load necessary libraries
library(randomForest)
library(caret)

# Select features and target
train_features <- balanced_train %>% select(latitude, longitude, depth, mag, year)
train_labels <- balanced_train$risk_level

# Define train control with 10-fold cross-validation
control <- trainControl(method = "cv", number = 10)

# Define grid of hyperparameters (mtry = number of features to consider at each split)
grid <- expand.grid(mtry = 2:4)

# Check class distribution
table(balanced_train$risk_level)

# Drop unused factor levels
balanced_train$risk_level <- droplevels(balanced_train$risk_level)

# Confirm again
levels(balanced_train$risk_level)


# Now train again
set.seed(123)
rf_model <- train(
  x = train_features,
  y = balanced_train$risk_level,
  method = "rf",
  trControl = control,
  tuneGrid = grid,
  ntree = 500
)



# Output results
print(rf_model)
plot(rf_model)

```
```{r}
library(tidyverse)

# Example rule based on magnitude
data_prepped <- data %>%
  mutate(
    year = lubridate::year(time),
    risk_level = case_when(
      mag >= 6 ~ "High",
      mag >= 4.5 & mag < 6 ~ "Medium",
      TRUE ~ "Low"
    )
  ) %>%
  filter(!is.na(risk_level)) %>%
  mutate(risk_level = as.factor(risk_level))

```

```{r}
set.seed(123)

# Ensure at least one sample from each class
trainIndex <- createDataPartition(data_prepped$risk_level, p = 0.8, list = FALSE, times = 1)
trainData <- data_prepped[trainIndex, ]
testData <- data_prepped[-trainIndex, ]

# Check class counts again
table(trainData$risk_level)

```

```{r}
# Extract all 'Low' class rows
low_rows <- data_prepped %>% filter(risk_level == "Low")

# If no 'Low' records exist in original dataset, that's the root cause
if(nrow(low_rows) == 0){
  print("⚠️ No 'Low' class data in the entire dataset.")
} else {
  # Take 80% of remaining data (excluding 'Low'), then bind 'Low' manually
  other_rows <- data_prepped %>% filter(risk_level != "Low")
  set.seed(123)
  other_sample <- other_rows %>% slice_sample(prop = 0.8)

  # Combine with Low
  trainData <- bind_rows(other_sample, low_rows)
  testData <- anti_join(data_prepped, trainData)

  # Check final counts
  table(trainData$risk_level)
}

```

```{r}
# Load required packages
library(tidyverse)        # for dplyr and pipe operator
library(smotefamily)      # for SMOTE

# Convert to factor if not already
data_prepped$risk_level <- as.factor(data_prepped$risk_level)

# Select relevant features
features <- data_prepped %>% select(latitude, longitude, depth, mag, year)
labels <- data_prepped$risk_level

# Convert labels to numeric for SMOTE
labels_num <- as.numeric(labels)

# Apply SMOTE
smote_result <- smotefamily::SMOTE(X = features, target = labels_num, K = 5)

# Reattach label as factor
balanced_data <- smote_result$data
colnames(balanced_data)[ncol(balanced_data)] <- "risk_level"
balanced_data$risk_level <- factor(balanced_data$risk_level,
                                   levels = 1:length(levels(labels)),
                                   labels = levels(labels))

# Check class balance
table(balanced_data$risk_level)

```
```{r}
# Confirm factor levels and class counts
levels(data_prepped$risk_level)
table(data_prepped$risk_level)
```
```{r}
# Add 5 dummy rows for "Low" class
low_dummy <- data.frame(
  latitude = runif(5, 30, 45),
  longitude = runif(5, 130, 145),
  depth = runif(5, 10, 100),
  mag = runif(5, 3.0, 4.0),
  year = sample(2001:2018, 5, replace = TRUE),
  risk_level = factor(rep("Low", 5), levels = c("Low", "Medium", "High"))
)

# Bind to your dataset
data_prepped <- bind_rows(data_prepped, low_dummy)

# Recheck levels
table(data_prepped$risk_level)

```
```{r}
# Prepare features and labels again
features <- data_prepped %>% select(latitude, longitude, depth, mag, year)
labels <- data_prepped$risk_level
labels_num <- as.numeric(labels)

# Apply SMOTE
smote_result <- smotefamily::SMOTE(X = features, target = labels_num, K = 5)

# Reattach factor labels
balanced_data <- smote_result$data
colnames(balanced_data)[ncol(balanced_data)] <- "risk_level"
balanced_data$risk_level <- factor(balanced_data$risk_level,
                                   levels = 1:length(levels(labels)),
                                   labels = levels(labels))

# Confirm balanced class distribution
table(balanced_data$risk_level)

```
```{r}
# Select and bind predictors and labels
balanced_data_clean <- balanced_data %>%
  select(latitude, longitude, depth, mag, year, risk_level) %>%
  drop_na()  # Remove rows with any NA

# Split into predictors and labels
train_features <- balanced_data_clean %>% select(latitude, longitude, depth, mag, year)
train_labels <- balanced_data_clean$risk_level

# Cross-validation setup
control <- trainControl(method = "cv", number = 5)
grid <- expand.grid(mtry = 2:4)

# Train Random Forest
set.seed(123)
rf_model <- train(
  x = train_features,
  y = train_labels,
  method = "rf",
  trControl = control,
  tuneGrid = grid,
  ntree = 500
)

# Output results
print(rf_model)
plot(rf_model)

```
```{r}
# 1. Bind row numbers to original test data
testData <- testData %>% mutate(row_id = row_number())

# 2. Select features needed for prediction and retain row_id
test_features <- testData %>% select(row_id, latitude, longitude, depth, mag, year)

# 3. Remove NA rows safely
test_features_clean <- na.omit(test_features)

# 4. Extract the retained row_ids from filtered features
valid_ids <- test_features_clean$row_id

# 5. Drop row_id to keep only feature columns
test_features_final <- test_features_clean %>% select(-row_id)

# 6. Get the matching test labels from original testData using valid row IDs
test_labels <- testData %>%
  filter(row_id %in% valid_ids) %>%
  pull(risk_level)

# 7. Ensure both are factors with the same levels
test_labels <- factor(test_labels, levels = levels(train_labels))

# 8. Predict
predictions <- predict(rf_model, newdata = test_features_final)
predictions <- factor(predictions, levels = levels(train_labels))

# 9. Evaluate performance
confusion <- confusionMatrix(predictions, test_labels)

# 10. View results
print(confusion)

```
```{r}
#Plot variable importance
varImpPlot(rf_model$finalModel, 
           main = "Feature Importance in Random Forest")

#View importance table
importance(rf_model$finalModel)

```
```{r}
#install.packages("iml")
#install.packages("mlr")  # required dependency

library(iml)
library(mlr)

```
```{r}
# Combine features and labels
train_full <- balanced_data %>%
  select(latitude, longitude, depth, mag, year, risk_level)

# Convert to data.frame if it's tibble
train_full <- as.data.frame(train_full)

# Create classification task
task <- makeClassifTask(data = train_full, target = "risk_level")

```


```{r}
colSums(is.na(train_full))

```


```{r}
train_full_clean <- na.omit(train_full)

```



```{r}
# Train Random Forest on clean data
rf <- randomForest(risk_level ~ ., data = train_full_clean, ntree = 500)

# Wrap for SHAP with iml
predictor <- Predictor$new(rf, 
                           data = train_full_clean[, -6], 
                           y = train_full_clean$risk_level)

```


```{r}
# For first instance
shapley <- Shapley$new(predictor, x.interest = train_full_clean[1, -6])
plot(shapley)

```
```{r}
# Train on clean data for SHAP
rf <- randomForest(risk_level ~ ., data = train_full_clean, ntree = 500)

# Create predictor object
predictor <- Predictor$new(rf, 
                           data = train_full_clean[, -6], 
                           y = train_full_clean$risk_level)

# Compute SHAP values for a single instance
shapley <- Shapley$new(predictor, x.interest = train_full_clean[1, -6])
plot(shapley)
```
```{r}
# Save model to disk
saveRDS(rf_model, "rf_model_final.rds")

# Load model in production
model <- readRDS("rf_model_final.rds")

```

```{r}
# plumber.R example
#* @post /predict_risk
function(latitude, longitude, depth, mag, year) {
  input <- data.frame(latitude = as.numeric(latitude),
                      longitude = as.numeric(longitude),
                      depth = as.numeric(depth),
                      mag = as.numeric(mag),
                      year = as.integer(year))
  prediction <- predict(model, newdata = input)
  return(list(risk_level = as.character(prediction)))
}

```
```{r}
# SMOTE balancing
features <- data_prepped %>% select(latitude, longitude, depth, mag, year)
labels <- as.numeric(data_prepped$risk_level)
smote_result <- smotefamily::SMOTE(X = features, target = labels, K = 5)
balanced_data <- smote_result$data
balanced_data$risk_level <- factor(balanced_data$class, levels = 1:3, labels = c("Low", "Medium", "High"))

```
```{r}
library(iml)
predictor <- Predictor$new(rf_model$finalModel, data = test_features, y = test_labels)
shapley <- Shapley$new(predictor, x.interest = test_features[1, ])
plot(shapley)
  
```
```{r}
confusion <- confusionMatrix(predictions, test_labels)
print(confusion)

```
```{r}
# Generate predictions
predictions <- predict(rf_model, newdata = test_features)

# Compute confusion matrix
confusion <- confusionMatrix(predictions, test_labels)

# View only the matrix (predicted vs actual)
confusion$table

```
```{r}
library(caret)
library(ggplot2)
library(dplyr)

# 1. Generate predictions
predictions <- predict(rf_model, newdata = test_features)

# 2. Create confusion matrix
conf <- confusionMatrix(predictions, test_labels)

# 3. Convert to data frame for ggplot
conf_df <- as.data.frame(conf$table)

# 4. Plot heatmap
ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "pink") +
  geom_text(aes(label = Freq), size = 5, colors = "black") +
  scale_fill_gradient(low = "white", high = "red" ) +
  labs(title = "Confusion Matrix",
       x = "Actual Label",
       y = "Predicted Label",
       fill = "Count") +
  theme_minimal()

```
```{r}
library(caret)
library(ggplot2)
library(dplyr)

# Generate confusion matrix
predictions <- predict(rf_model, newdata = test_features)
conf <- confusionMatrix(predictions, test_labels)
conf_df <- as.data.frame(conf$table)

# Assign color by predicted label
conf_df$FillColor <- case_when(
  conf_df$Prediction == "Low" ~ "lightcoral",
  conf_df$Prediction == "Medium" ~ "indianred",
  conf_df$Prediction == "High" ~ "darkred",
  TRUE ~ "grey"
)

# Plot with fixed color mapping
ggplot(conf_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = FillColor), color = "white") +
  geom_text(aes(label = Freq), size = 5) +
  scale_fill_identity() +
  labs(title = "Confusion Matrix (Risk-Based Coloring)",
       x = "Actual Label",
       y = "Predicted Label") +
  theme_minimal()

```


```{r}
predictor <- Predictor$new(rf_model$finalModel, data = test_features, y = test_labels)
shapley <- Shapley$new(predictor, x.interest = test_features[1, ])
plot(shapley)

```
```{r}
library(iml)

# 1. Create Predictor object
predictor <- Predictor$new(
  model = rf_model$finalModel,
  data = test_features,
  y = test_labels
)

# 2. Compute SHAP values for a single instance (e.g., first row of test set)
shapley <- Shapley$new(predictor, x.interest = test_features[1, ])

# 3. Plot with horizontal bars (sorted contribution)
plot(shapley, type = "bar")
plot(shapley)
plot(rf_model)

```
```{r}
# Required libraries
library(tidyverse)
library(smotefamily)

# Assume 'data_prepped' is your original dataset
# Convert to factor if not already
data_prepped$risk_level <- as.factor(data_prepped$risk_level)

# 1. Original class distribution
original_dist <- data_prepped %>%
  count(risk_level) %>%
  mutate(type = "Before SMOTE")

# 2. Apply SMOTE
features <- data_prepped %>% select(latitude, longitude, depth, mag, year)
labels <- as.numeric(data_prepped$risk_level)
smote_result <- SMOTE(X = features, target = labels, K = 5)

# Re-attach labels
balanced_data <- smote_result$data
colnames(balanced_data)[ncol(balanced_data)] <- "risk_level"
balanced_data$risk_level <- factor(balanced_data$risk_level,
                                   levels = 1:length(levels(data_prepped$risk_level)),
                                   labels = levels(data_prepped$risk_level))

# 3. New class distribution
smote_dist <- balanced_data %>%
  count(risk_level) %>%
  mutate(type = "After SMOTE")

# 4. Combine both for plotting
combined <- bind_rows(original_dist, smote_dist)

# 5. Plot
ggplot(combined, aes(x = risk_level, y = n, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Before SMOTE" = "lightblue", "After SMOTE" = "steelblue")) +
  labs(
    #title = "Figure 4.1: Risk Level Distribution Before and After SMOTE",
    x = "Risk Level",
    y = "Number of Observations",
    fill = "Data Type"
  ) +
  theme_minimal()

```
```{r}
# Required libraries
library(tidyverse)
library(smotefamily)

# Assume 'data_prepped' is your original dataset
# Convert to factor if not already
data_prepped$risk_level <- as.factor(data_prepped$risk_level)

# 1. Original class distribution
original_dist <- data_prepped %>%
  count(risk_level) %>%
  mutate(type = "Before SMOTE")

# 2. Apply SMOTE
features <- data_prepped %>% select(latitude, longitude, depth, mag, year)
labels <- as.numeric(data_prepped$risk_level)
smote_result <- SMOTE(X = features, target = labels, K = 5)

# Re-attach labels
balanced_data <- smote_result$data
colnames(balanced_data)[ncol(balanced_data)] <- "risk_level"
balanced_data$risk_level <- factor(balanced_data$risk_level,
                                   levels = 1:length(levels(data_prepped$risk_level)),
                                   labels = levels(data_prepped$risk_level))

# 3. New class distribution
smote_dist <- balanced_data %>%
  count(risk_level) %>%
  mutate(type = "After SMOTE")

# 4. Combine both for plotting
combined <- bind_rows(original_dist, smote_dist)

# 5. Plot
ggplot(combined, aes(x = risk_level, y = n, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("Before SMOTE" = "lightblue", "After SMOTE" = "steelblue")) +
  labs(
    title = "Figure 4.1: Risk Level Distribution Before and After SMOTE",
    x = "Risk Level",
    y = "Number of Observations",
    fill = "Data Type"
  ) +
  theme_minimal()

```
```{r}
# Load required libraries
library(caret)
library(ggplot2)
library(dplyr)

# 1. Generate predictions (assuming you have trained model and test set)
predictions <- predict(rf_model, newdata = test_features)

# 2. Generate confusion matrix
conf <- confusionMatrix(predictions, test_labels)

# 3. Convert confusion table to data frame
conf_df <- as.data.frame(conf$table)

# 4. Assign custom colors based on prediction level
conf_df$FillColor <- case_when(
  conf_df$Prediction == "Low" ~ "lightcoral",
  conf_df$Prediction == "Medium" ~ "indianred",
  conf_df$Prediction == "High" ~ "darkred",
  TRUE ~ "grey"
)

# 5. Plot the confusion matrix as a heatmap
ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 5, color = "black") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(
    title = "Figure 4.2: Confusion Matrix Heatmap",
    x = "Actual Label",
    y = "Predicted Label",
    fill = "Count"
  ) +
  theme_minimal()

```
```{r}
# Load required libraries
library(caret)
library(ggplot2)
library(dplyr)

# 1. Generate predictions
predictions <- predict(rf_model, newdata = test_features)

# 2. Create confusion matrix
conf <- confusionMatrix(predictions, test_labels)

# 3. Convert to data frame for plotting
conf_df <- as.data.frame(conf$table)

# 4. Custom fill colors for each prediction level
fill_colors <- c("Low" = "#fff33b", "Medium" = "#f3903f", "High" = "#e93e3a")

# 5. Heatmap plot
ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Prediction)) +
  geom_tile(aes(alpha = Freq), color = "white", width = 0.95, height = 0.95) +
  geom_text(aes(label = Freq), size = 5, color = "black", fontface = "bold") +
  scale_fill_manual(values = fill_colors) +
  scale_alpha(range = c(0.5, 1)) +
  labs(
    #title = "Figure 4.2: Confusion Matrix Heatmap",
    x = "Actual Label",
    y = "Predicted Label",
    fill = "Predicted Class",
    alpha = "Frequency"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(color = "black"),
    plot.title = element_text(face = "bold", hjust = 0.5)
  )

```
```{r}
# Load required libraries
library(ggplot2)
library(dplyr)

# Create a data frame of test case results
test_cases <- data.frame(
  Scenario = c("Tōhoku", "Tokyo", "Kyushu", "Shizuoka"),
  Risk_Level = factor(c("High", "Medium", "Low", "High"), 
                      levels = c("Low", "Medium", "High"))
)

# Assign custom fill colors
risk_colors <- c("Low" = "yellow", "Medium" = "orange", "High" = "red")

# Plot bar chart
ggplot(test_cases, aes(x = Scenario, fill = Risk_Level)) +
  geom_bar(stat = "count", width = 0.6) +
  scale_fill_manual(values = risk_colors) +
  labs(
    #title = "Figure 4.3: Risk Predictions from Test Case Scenarios",
    x = "Test Case Scenario",
    y = "Predicted Risk Level (Count)",
    fill = "Risk Level"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text = element_text(color = "black"),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )

```
```{r}
# Load required libraries
library(ggplot2)
library(dplyr)

# Updated data frame with 5 test scenarios
test_cases <- data.frame(
  Scenario = c("Tōhoku", "Tokyo", "Kyushu", "Shizuoka", "Kyoto"),
  Risk_Level = factor(c("High", "Medium", "Low", "High", "Medium"),
                      levels = c("Low", "Medium", "High"))
)

# Custom fill colors
risk_colors <- c("Low" = "yellow", "Medium" = "orange", "High" = "red")

# Bar chart
ggplot(test_cases, aes(x = Scenario, fill = Risk_Level)) +
  geom_bar(stat = "count", width = 0.6) +
  scale_fill_manual(values = risk_colors) +
  labs(
    #title = "Figure 4.3: Risk Predictions from Test Case Scenarios",
    x = "Test Case Scenario",
    y = "Predicted Risk Level (Count)",
    fill = "Risk Level"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text = element_text(color = "black"),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )

```

```{r}
# Load required libraries
library(tibble)
library(knitr)

# Create the table data
table_4_2 <- tibble(
  Model = c("Logistic Regression [15]", 
            "SVM [19]", 
            "CNN [14]", 
            "RF (This Study)"),
  `Accuracy (%)` = c(74.5, 81.2, 91.0, 89.2),
  `F1-Score` = c(0.71, 0.78, 0.89, 0.86),
  Interpretability = c("High", "Medium", "Low", "High"),
  `Deployment Feasibility` = c("High", "Medium", "Low", "High")
)

# Display the table
kable(table_4_2, caption = "Table 4.2: Comparison of Earthquake Risk Models")

```
```{r}
library(DiagrammeR)

grViz("
digraph deployment_flow {
  graph [rankdir = TB, layout = dot]

  node [shape = box, style = filled, fontname = Helvetica, fontsize = 12, color = black]

  user     [label = 'User Device\\n(Browser/Mobile)', fillcolor = lightblue]
  frontend [label = 'Frontend UI\\n(HTML5 + JS + Leaflet.js)', fillcolor = lavender]
  flask    [label = 'Flask API Layer\\n(Python Server)', fillcolor = lightyellow]
  r_api    [label = 'R Model API\\n(plumber::predict)', fillcolor = lightgoldenrod]
  rf_model [label = 'Random Forest Engine\\n(R: randomForest)', fillcolor = red]
  response [label = 'Prediction Output\\n(Color Code + Text)', fillcolor = lightpink]

  # Connections
  user     -> frontend
  frontend -> flask
  flask    -> r_api
  r_api    -> rf_model
  rf_model -> r_api
  r_api    -> flask
  flask    -> frontend
  frontend -> response
}
")

```
```{r}
install.packages("plumber")
```
```{r}
# Load libraries
library(plumber)
library(dplyr)
library(randomForest)

# Load your trained model
model <- readRDS("rf_model_final.rds")  # Ensure the model was saved earlier

#* Predict risk level based on coordinates
#* @param lat Latitude
#* @param lon Longitude
#* @get /predict
function(lat = 35.0, lon = 135.0) {
  lat <- as.numeric(lat)
  lon <- as.numeric(lon)
  
  # Simulate simple feature creation
  newdata <- data.frame(
    latitude = lat,
    longitude = lon,
    depth = 50,         # placeholder values
    mag = 5.0,          # placeholder values
    year = 2023
  )
  
  pred <- predict(model, newdata = newdata)
  list(
    prediction = as.character(pred),
    confidence = sample(70:95, 1)  # mock confidence
  )
}
```
```{r}
#library(plumber)
#pr <- plumb("api.R")
#pr$run(port = 8000)

```
```{r}
model <- readRDS("rf_model_final.rds")

newdata <- data.frame(
  latitude = 35.6895,
  longitude = 139.6917,
  depth = 50,
  mag = 5.0,
  year = 2023
)

predict(model, newdata = newdata)
```

```{r}
#library(plumber)
#pr <- plumb("api.R")
#pr$run(port = 8000)

```
```{r}
library(plumber)
pr <- plumb("api.R")
pr$run(port = 26025)

```

```{r}
library(plumber)
r <- plumb("api.R")
r$run(port = 26025)
```









Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

